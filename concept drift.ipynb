{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fake_norm_dateset(column_size=20, instance_size=100000):\n",
    "    \"\"\"\n",
    "    Input size: total batch size\n",
    "    Distribution: gen a fake dataset for test, 20 coloumns is normal distributaion.\n",
    "    \"\"\"\n",
    "    dataset = {}\n",
    "    for i in range(column_size):\n",
    "        dataset['col_{}'.format(i)] = np.random.normal(0,1,instance_size)\n",
    "    df = pd.DataFrame(dataset)\n",
    "    train = df[:instance_size//2]\n",
    "    test = df[instance_size//2:]\n",
    "    # add drift to column 0\n",
    "    test['col_0'] += np.random.normal(0.1,0.5,len(test))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baomj/.local/lib/python3.5/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "batch1, batch2 = gen_fake_norm_dateset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size, random_state=2018):\n",
    "    \"\"\"\n",
    "    split data to train and test\n",
    "    \"\"\"\n",
    "    sss = list(StratifiedShuffleSplit(\n",
    "        n_splits=1, test_size=test_size, random_state=random_state).split(X, y))\n",
    "    X_train = np.take(X, sss[0][0], axis=0)\n",
    "    X_test = np.take(X, sss[0][1], axis=0)\n",
    "    y_train = np.take(y, sss[0][0], axis=0)\n",
    "    y_test = np.take(y, sss[0][1], axis=0)\n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fea_importance(clf, feature_name):\n",
    "    \"\"\"\n",
    "    get feature importance from lightGBM\n",
    "    \"\"\"\n",
    "    gain = clf.feature_importance('gain')\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature':clf.feature_name(),\n",
    "        'split': clf.feature_importance('split'),\n",
    "        'gain': gain, # * gain / gain.sum(),\n",
    "        'gain_percent':100 *gain / gain.sum(),\n",
    "        }).sort_values('gain',ascending=False)\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversial_validation(batch1, batch2):\n",
    "    \"\"\"\n",
    "    split two batch to get importance\n",
    "    \"\"\"\n",
    "    feature_name = list(batch1.columns)\n",
    "    train_X = batch1\n",
    "    train_Y = np.ones(train_X.shape[0])\n",
    "    test_X = batch2\n",
    "    test_Y = np.zeros(test_X.shape[0])\n",
    "    X = np.concatenate((train_X.values,test_X.values),axis=0)\n",
    "    y = np.concatenate((train_Y,test_Y),axis=0)\n",
    "    test_size = int(len(X)/5) \n",
    "    X, X_test, y, y_test = train_test_split(X, y, test_size, random_state = 42)\n",
    "    para = {\n",
    "        'num_leaves': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'bagging_fraction': 0.2, \n",
    "        'feature_fraction': 0.5,\n",
    "        'max_depth': 3, \n",
    "        \"objective\": \"binary\", \n",
    "        \"metric\":\"auc\", \n",
    "        'verbose': -1, \n",
    "        \"seed\": 42, \n",
    "        'num_threads': 8,\n",
    "    }\n",
    "    lgb_train = lgb.Dataset(X, y, free_raw_data=True)\n",
    "    lgb_val = lgb.Dataset(X_test, y_test, free_raw_data=True, reference=lgb_train)\n",
    "    lgb_model = lgb.train(para, lgb_train, valid_sets=lgb_val, valid_names='eval',feature_name=feature_name,\n",
    "                                verbose_eval=False, early_stopping_rounds=10, num_boost_round=50)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        y_test, lgb_model.predict(X_test, num_iteration = lgb_model.best_iteration))\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"----Adversial Score is {}------\".format(auc))\n",
    "    fea_importance_adversial = get_fea_importance(lgb_model, feature_name)\n",
    "    print(fea_importance_adversial.head(10))\n",
    "    return fea_importance_adversial, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the batch split result, feature importance and auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Adversial Score is 0.5430079299999999------\n",
      "   feature         gain  gain_percent  split\n",
      "0    col_0  1535.210706     82.929543     13\n",
      "8   col_16    39.181170      2.116502      5\n",
      "6   col_14    37.343861      2.017254      4\n",
      "7   col_15    32.069911      1.732364      3\n",
      "19   col_9    29.686660      1.603624      3\n",
      "4   col_12    27.492129      1.485079      3\n",
      "13   col_3    24.361560      1.315971      3\n",
      "17   col_7    23.504041      1.269649      2\n",
      "18   col_8    22.153960      1.196720      3\n",
      "3   col_11    20.220530      1.092280      3\n"
     ]
    }
   ],
   "source": [
    "fea_imp, auc_true = adversial_validation(batch1, batch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the threshold. We could run more to get a distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Adversial Score is 0.5032942699999999------\n",
      "   feature       gain  gain_percent  split\n",
      "6   col_13  28.866321     56.871471      3\n",
      "4   col_11  11.377600     22.415771      1\n",
      "1    col_0  10.513200     20.712759      1\n",
      "0    index   0.000000      0.000000      0\n",
      "12  col_19   0.000000      0.000000      0\n",
      "19   col_8   0.000000      0.000000      0\n",
      "18   col_7   0.000000      0.000000      0\n",
      "17   col_6   0.000000      0.000000      0\n",
      "16   col_5   0.000000      0.000000      0\n",
      "15   col_4   0.000000      0.000000      0\n",
      "----Adversial Score is 0.49869012499999993------\n",
      "   feature       gain  gain_percent  split\n",
      "14   col_3  27.136200     29.117084      2\n",
      "7   col_14  15.723741     16.871540      2\n",
      "12  col_19  11.812400     12.674680      1\n",
      "11  col_18   8.082130      8.672109      1\n",
      "13   col_2   8.061890      8.650390      1\n",
      "8   col_15   7.570190      8.122798      1\n",
      "15   col_4   7.545360      8.096155      1\n",
      "1    col_0   7.264920      7.795244      1\n",
      "0    index   0.000000      0.000000      0\n",
      "19   col_8   0.000000      0.000000      0\n",
      "----Adversial Score is 0.5060458999999999------\n",
      "   feature       gain  gain_percent  split\n",
      "3   col_10  19.209721     14.511963      2\n",
      "16   col_5  17.967450     13.573491      2\n",
      "5   col_12  11.547500      8.723546      1\n",
      "4   col_11  10.333900      7.806734      1\n",
      "1    col_0   9.639920      7.282467      1\n",
      "14   col_3   9.133960      6.900240      1\n",
      "12  col_19   9.093020      6.869313      1\n",
      "6   col_13   8.772120      6.626889      1\n",
      "17   col_6   8.561660      6.467897      1\n",
      "11  col_18   7.826820      5.912763      1\n",
      "----Adversial Score is 0.5034411149999999------\n",
      "   feature      gain  gain_percent  split\n",
      "6   col_13  29.17188     33.567354      3\n",
      "17   col_6   9.94613     11.444763      1\n",
      "15   col_4   8.69438     10.004406      1\n",
      "11  col_18   8.15450      9.383180      1\n",
      "13   col_2   8.14075      9.367358      1\n",
      "16   col_5   8.11010      9.332089      1\n",
      "4   col_11   7.94440      9.141423      1\n",
      "8   col_15   6.74337      7.759428      1\n",
      "0    index   0.00000      0.000000      0\n",
      "19   col_8   0.00000      0.000000      0\n",
      "----Adversial Score is 0.501775715------\n",
      "   feature      gain  gain_percent  split\n",
      "17   col_6  14.37020     15.415790      1\n",
      "12  col_19  10.03780     10.768160      1\n",
      "4   col_11   9.91918     10.640909      1\n",
      "11  col_18   9.19331      9.862224      1\n",
      "16   col_5   8.89788      9.545298      1\n",
      "13   col_2   8.78353      9.422629      1\n",
      "8   col_15   8.52893      9.149503      1\n",
      "6   col_13   8.08923      8.677810      1\n",
      "7   col_14   7.70004      8.260302      1\n",
      "5   col_12   7.69731      8.257374      1\n"
     ]
    }
   ],
   "source": [
    "estimate_thres_auc = []\n",
    "estimate_thres_gain = []\n",
    "for i in range(5):\n",
    "    len_batch1 = len(batch1) \n",
    "    base_df = batch1.append(batch2).reset_index(drop = False).sample(frac=1)\n",
    "    fea_base, auc_base = adversial_validation(base_df[:len_batch1], base_df[len_batch1:])\n",
    "    estimate_thres_auc.append(auc_base)\n",
    "    estimate_thres_gain.append(fea_base['gain'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5026494249999999"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#auc threashold\n",
    "np.mean(estimate_thres_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.750864219665527"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drift threashold\n",
    "np.mean(estimate_thres_gain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
